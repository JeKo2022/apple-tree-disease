{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Milestone3_MLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeLaKo/apple-tree-disease/blob/main/Milestone4_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the data and importing the necessary libraries"
      ],
      "metadata": {
        "id": "TqzOGBFWtYKn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NF6qQ5qUmQI_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b8e96f-b24a-420a-b988-5fb4db032826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import math as tfmath\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "ibOekLk5tlko",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be3c0485-d8a1-4666-d839-39c69b4ac20d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/MLP/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import mlxtend package for confusion matrix\n",
        "import mlxtend\n",
        "                                                          \n",
        "print(mlxtend.__version__) \n",
        "\n",
        "! pip install mlxtend --upgrade --no-deps\n",
        "\n",
        "print(mlxtend.__version__) \n",
        "\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dHkG28jM9N9",
        "outputId": "76179393-42da-449b-9441-83730860756d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.19.0\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.7/dist-packages (0.19.0)\n",
            "0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Only run the following cells if data needs to be newly downloaded !\n",
        "'''\n",
        "# !pip install -U -q kaggle==1.5.8"
      ],
      "metadata": {
        "id": "ciuScTfXr0VL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/MLP/\"\n",
        "# ! kaggle competitions download -c plant-pathology-2021-fgvc8"
      ],
      "metadata": {
        "id": "XIrQYT90uv5m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/gdrive/MyDrive/MLP/"
      ],
      "metadata": {
        "id": "CFd-yrB9XWzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! mkdir train"
      ],
      "metadata": {
        "id": "CnpAjve8XtVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! unzip -q plant-pathology-2021-fgvc8.zip -d train"
      ],
      "metadata": {
        "id": "4w4s3qRQ3Z6X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data processing\n"
      ],
      "metadata": {
        "id": "v192m162tVHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/MLP/train/"
      ],
      "metadata": {
        "id": "IesKjmS1cNkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read training data\n",
        "df = pd.read_csv('train.csv')"
      ],
      "metadata": {
        "id": "4oGL6rPetoLs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change combined classes to only contain a single label\n",
        "new_labels = df['labels'].to_list()\n",
        "\n",
        "for i in range(len(new_labels)):\n",
        "  if new_labels[i] == 'scab frog_eye_leaf_spot complex' or new_labels[i] == 'scab frog_eye_leaf_spot':\n",
        "    new_labels[i] = 'scab'\n",
        "  elif new_labels[i] == 'frog_eye_leaf_spot complex':\n",
        "    new_labels[i] = 'frog_eye_leaf_spot'\n",
        "  elif new_labels[i] == 'powdery_mildew complex':\n",
        "    new_labels[i] = 'powdery_mildew'\n",
        "  elif new_labels[i] == 'rust complex' or new_labels[i] == 'rust frog_eye_leaf_spot':\n",
        "    new_labels[i] = 'rust'\n",
        "\n",
        "# replace labels with adjusted labels in dataframe\n",
        "df['adjusted labels'] = np.array(new_labels)\n",
        "df = df.drop('labels', axis = 1)"
      ],
      "metadata": {
        "id": "MKIFbUN5mLzA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(df, sample_size):\n",
        "  \"\"\"\n",
        "  This function gets an equal sample inclusive of all classes from the input dataframe\n",
        "  \"\"\"\n",
        "  df_sampled = []\n",
        "  classes = df['adjusted labels'].unique()\n",
        "\n",
        "  for i in classes:\n",
        "      g = df[df['adjusted labels'] == i].sample(sample_size)\n",
        "      df_sampled.append(g)\n",
        "\n",
        "  df_sampled = pd.concat(df_sampled)\n",
        "  return df_sampled\n",
        "\n",
        "# select sample from dataframe\n",
        "df_sampled = sample(df, 1184)\n",
        "print(df_sampled.head(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh7Rr6dNzlMN",
        "outputId": "a35b4e57-2727-4db9-c5dd-4efde95a5c8d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      image adjusted labels\n",
            "17602  faaa83b7f029445a.jpg         healthy\n",
            "11495  d0783ec96abc4a4d.jpg         healthy\n",
            "5011   a72ce33a86f60be0.jpg         healthy\n",
            "14342  e4a55870fc9f1394.jpg         healthy\n",
            "17386  f909db508fe0867c.jpg         healthy\n",
            "...                     ...             ...\n",
            "2272   924dd81aed8372cd.jpg  powdery_mildew\n",
            "10027  c56a369b0bd0e0f5.jpg  powdery_mildew\n",
            "3155   9896e68da7e59524.jpg  powdery_mildew\n",
            "17846  fc542b37e0e79940.jpg  powdery_mildew\n",
            "3770   9f946a2a2d38d5b4.jpg  powdery_mildew\n",
            "\n",
            "[7104 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(df):\n",
        "  \"\"\"\n",
        "  This function returns all classes and combination of classes found in the input-dataframe, and returns\n",
        "  the one-hot encoded version\n",
        "  \"\"\"\n",
        "  one_hot = pd.get_dummies(df['adjusted labels'])\n",
        "  df = df.drop('adjusted labels', axis = 1)\n",
        "  df = df.join(one_hot)\n",
        "  return df\n",
        "\n",
        "# convert labels of dataframe to one-hot encoded classes\n",
        "df_onehot = one_hot(df_sampled)\n",
        "print(df_onehot.head(2))"
      ],
      "metadata": {
        "id": "GdWniTfft-GO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image-data processing"
      ],
      "metadata": {
        "id": "R3ibsTmfcdxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Only run this cell if images need resizing !\n",
        "'''\n",
        "! mkdir resized_train_images\n",
        "\n",
        "# gather names of all images in the image-directory\n",
        "train_images = os.listdir('train_images/')\n",
        "\n",
        "# resize all images and save it to a new directory\n",
        "# for faster performance\n",
        "for image in train_images:\n",
        "  img = cv2.imread('train_images/' + image)\n",
        "  resized_img = cv2.resize(img, (96, 96)) \n",
        "  cv2.imwrite('resized_train_images/' + image, resized_img)\n"
      ],
      "metadata": {
        "id": "4jts4lwY0YS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert dataframe to a hashable list; dictionary\n",
        "df_dict = df_onehot.set_index('image').T.to_dict('list')\n",
        "\n",
        "# gather names of all images in the resized directory\n",
        "resized_images = os.listdir('resized_train_images/')\n",
        "\n",
        "images = []\n",
        "# find corresponding image from the resized directory to the selected sample found\n",
        "# in dictionary and add that to a list\n",
        "for image in df_dict.keys():\n",
        "  if image in resized_images:\n",
        "    img_resized = cv2.imread('resized_train_images/' + image) \n",
        "    images.append(img_resized)"
      ],
      "metadata": {
        "id": "ksV1F9z-I7xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert image list to array\n",
        "images = np.array(images)\n",
        "\n",
        "# convert dictionary values to array\n",
        "labels = np.array(list(df_dict.values()))\n",
        "\n",
        "# split data\n",
        "x_train, x_val, y_train, y_val = train_test_split(images, labels, test_size = 0.3, random_state=42)"
      ],
      "metadata": {
        "id": "o4A7xjIFAwyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building and training the network"
      ],
      "metadata": {
        "id": "2Zhu6lDvWjq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import layers, models, preprocessing\n",
        "\n",
        "def train_and_evaluate(model, train_x, train_y, val_x, val_y, preprocess={}, epochs=20, augment={}):\n",
        "\n",
        "    # optimizer = keras.optimizers.Adam(lr = 0.01)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
        "\n",
        "    train_gen = preprocessing.image.ImageDataGenerator(**preprocess, **augment)\n",
        "    train_gen.fit(train_x) \n",
        "\n",
        "    val_gen = preprocessing.image.ImageDataGenerator(**preprocess)\n",
        "    val_gen.fit(train_x)\n",
        "\n",
        "    history = model.fit(train_gen.flow(train_x, train_y), epochs=epochs, \n",
        "                        validation_data=val_gen.flow(val_x, val_y))\n",
        "\n",
        "    fig, axs = plt.subplots(1,2,figsize=(20,5)) \n",
        "\n",
        "    for i, metric in enumerate(['loss', 'accuracy']):\n",
        "        axs[i].plot(history.history[metric])\n",
        "        axs[i].plot(history.history['val_'+metric])\n",
        "        axs[i].legend(['training', 'validation'], loc='best')\n",
        "\n",
        "        axs[i].set_title('Model '+metric)\n",
        "        axs[i].set_ylabel(metric)\n",
        "        axs[i].set_xlabel('epoch')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Validation Accuracy: {model.evaluate(val_gen.flow(val_x, val_y))[1]}\")"
      ],
      "metadata": {
        "id": "2uvdJ8Yh3W7g"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML MODEL ARCHITECTURE\n",
        "# Define Sequential model\n",
        "model = models.Sequential()\n",
        "\n",
        "# create convolutional layer and max pooling layer\n",
        "model.add(layers.Conv2D(32, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.5), padding='same', input_shape=(96, 96, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# create convolutional layer (larger) and max pooling layer\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.5), padding='same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "# add Conv2D layer with 128 filters and max pooling layer\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.5), padding='same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "# add Conv2D layer with 256 filters and max pooling layer\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(layers.Conv2D(256, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.5), padding='same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# add Conv2D layer with 32 filters and max pooling layer\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.5), padding='same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# flatten layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(layers.Dense(256, activation=tf.keras.layers.LeakyReLU(alpha=0.5)))\n",
        "\n",
        "# apply softmax activation for final layer classification\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(layers.Dense(6, activation='softmax'))\n",
        "\n",
        "# normalize input data: set preprocesing dictionary\n",
        "preprocess = {'featurewise_center': True, 'featurewise_std_normalization' : True}\n",
        "\n",
        "# augment data: set augmentation dictionary\n",
        "augment = {'horizontal_flip': True, \n",
        "           'vertical_flip': True, \n",
        "           'rotation_range': 20, \n",
        "           'width_shift_range': 0.1, \n",
        "           'height_shift_range': 0.1, \n",
        "           'zoom_range': [0.3,1.0], \n",
        "           'brightness_range': [0.2,1.2],\n",
        "           'channel_shift_range' : 0.7}\n",
        "\n",
        "# run training and evaluation function\n",
        "train_and_evaluate(model, x_train, y_train, x_val, y_val, preprocess, epochs = 80, augment = augment)\n"
      ],
      "metadata": {
        "id": "ULlZim8l2zKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "aEjUnYrJ6u2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix"
      ],
      "metadata": {
        "id": "lvYNMVddWur6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select classes\n",
        "classes = df['adjusted labels'].unique()\n",
        "display(classes)\n",
        "\n",
        "# gather actual and predicted classes\n",
        "y_true = tf.argmax(y_val, axis=1)\n",
        "# y_pred = tf.keras.utils.to_categorical(y_pred, num_classes = 6)\n",
        "y_pred = tf.argmax(model.predict(x_val), axis = 1)\n",
        "\n",
        "# plot confusion matrix 1\n",
        "conf_matrix = tfmath.confusion_matrix(y_true, y_pred, num_classes = 6)\n",
        "\n",
        "ax = sns.heatmap(conf_matrix, xticklabels=classes, yticklabels=classes)\n",
        "ax.set(xlabel='Predicted Class', ylabel='Actual Class')\n",
        "plt.show()\n",
        "\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "jcwt60b_Jtc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot confusion matrix 2\n",
        "mtrx = confusion_matrix(y_true, y_pred)\n",
        "plot_confusion_matrix(conf_mat = mtrx, figsize=(8, 8), class_names=classes, colorbar=True, show_normed = True)"
      ],
      "metadata": {
        "id": "wRrn2C6vNU6q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}