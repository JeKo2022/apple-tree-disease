{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Milestone3_MLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeLaKo/apple-tree-disease/blob/main/Milestone4_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the data and importing the necessary libraries"
      ],
      "metadata": {
        "id": "TqzOGBFWtYKn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NF6qQ5qUmQI_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9de2e98a-e164-476f-cd8e-f9f5b9f86158"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import math as tfmath\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "ibOekLk5tlko"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import mlxtend package for confusion matrix\n",
        "import mlxtend\n",
        "                                                          \n",
        "print(mlxtend.__version__) \n",
        "\n",
        "! pip install mlxtend --upgrade --no-deps\n",
        "\n",
        "print(mlxtend.__version__) \n",
        "\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dHkG28jM9N9",
        "outputId": "39448894-639c-4083-a1d6-01f2f2ded143"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.19.0\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.7/dist-packages (0.19.0)\n",
            "0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Only run the following cells if data needs to be newly downloaded !\n",
        "'''\n",
        "# !pip install -U -q kaggle==1.5.8"
      ],
      "metadata": {
        "id": "ciuScTfXr0VL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e10b4aa4-f73a-40ea-9d4f-12f5f30cad96"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nOnly run the following cells if data needs to be newly downloaded !\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/MLP/\"\n",
        "# ! kaggle competitions download -c plant-pathology-2021-fgvc8"
      ],
      "metadata": {
        "id": "XIrQYT90uv5m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/gdrive/MyDrive/MLP/"
      ],
      "metadata": {
        "id": "CFd-yrB9XWzT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! mkdir train"
      ],
      "metadata": {
        "id": "CnpAjve8XtVx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! unzip -q plant-pathology-2021-fgvc8.zip -d train"
      ],
      "metadata": {
        "id": "4w4s3qRQ3Z6X"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data processing\n"
      ],
      "metadata": {
        "id": "v192m162tVHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/MLP/train/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IesKjmS1cNkU",
        "outputId": "ab9d3fa0-5cbc-4dc0-c8e6-3fb11bbc483d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/MLP/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read training data\n",
        "df = pd.read_csv('train.csv')"
      ],
      "metadata": {
        "id": "4oGL6rPetoLs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change combined classes to only contain a single label\n",
        "new_labels = df['labels'].to_list()\n",
        "\n",
        "for i in range(len(new_labels)):\n",
        "  if new_labels[i] == 'scab frog_eye_leaf_spot complex' or new_labels[i] == 'scab frog_eye_leaf_spot':\n",
        "    new_labels[i] = 'scab'\n",
        "  elif new_labels[i] == 'frog_eye_leaf_spot complex':\n",
        "    new_labels[i] = 'frog_eye_leaf_spot'\n",
        "  elif new_labels[i] == 'powdery_mildew complex':\n",
        "    new_labels[i] = 'powdery_mildew'\n",
        "  elif new_labels[i] == 'rust complex' or new_labels[i] == 'rust frog_eye_leaf_spot':\n",
        "    new_labels[i] = 'rust'\n",
        "\n",
        "# replace labels with adjusted labels in dataframe\n",
        "df['adjusted labels'] = np.array(new_labels)\n",
        "df = df.drop('labels', axis = 1)"
      ],
      "metadata": {
        "id": "MKIFbUN5mLzA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(df, sample_size):\n",
        "  \"\"\"\n",
        "  This function gets an equal sample inclusive of all classes from the input dataframe\n",
        "  \"\"\"\n",
        "  df_sampled = []\n",
        "  classes = df['adjusted labels'].unique()\n",
        "\n",
        "  for i in classes:\n",
        "      g = df[df['adjusted labels'] == i].sample(sample_size)\n",
        "      df_sampled.append(g)\n",
        "\n",
        "  df_sampled = pd.concat(df_sampled)\n",
        "  return df_sampled\n",
        "\n",
        "# select sample from dataframe\n",
        "df_sampled = sample(df, 1184)\n",
        "print(df_sampled.head(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh7Rr6dNzlMN",
        "outputId": "1e0cc4e1-749d-4fd9-d618-75c496869a75"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     image adjusted labels\n",
            "1274  899ebda59584bc98.jpg         healthy\n",
            "2950  96b60cbdacd5948a.jpg         healthy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(df):\n",
        "  \"\"\"\n",
        "  This function returns all classes and combination of classes found in the input-dataframe, and returns\n",
        "  the one-hot encoded version\n",
        "  \"\"\"\n",
        "  one_hot = pd.get_dummies(df['adjusted labels'])\n",
        "  df = df.drop('adjusted labels', axis = 1)\n",
        "  df = df.join(one_hot)\n",
        "  return df\n",
        "\n",
        "# convert labels within dataframe to one-hot encoded classes\n",
        "df_onehot = one_hot(df_sampled)\n",
        "print(df_onehot.head(2))"
      ],
      "metadata": {
        "id": "GdWniTfft-GO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03f3dc7d-e3ac-4a4c-951f-00e3350f62c0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     image  complex  ...  rust  scab\n",
            "1274  899ebda59584bc98.jpg        0  ...     0     0\n",
            "2950  96b60cbdacd5948a.jpg        0  ...     0     0\n",
            "\n",
            "[2 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image-data processing"
      ],
      "metadata": {
        "id": "R3ibsTmfcdxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Only run this cell if images need resizing !\n",
        "'''\n",
        "# create new directory\n",
        "! mkdir resized_train_images\n",
        "\n",
        "# gather names of all images in the image-directory\n",
        "train_images = os.listdir('train_images/')\n",
        "\n",
        "# resize all images and save it to a new directory \n",
        "for image in train_images:\n",
        "  img = cv2.imread('train_images/' + image)\n",
        "  resized_img = cv2.resize(img, (96, 96)) \n",
        "  cv2.imwrite('resized_train_images/' + image, resized_img)\n"
      ],
      "metadata": {
        "id": "4jts4lwY0YS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert dataframe to a hashable list; dictionary\n",
        "df_dict = df_onehot.set_index('image').T.to_dict('list')\n",
        "\n",
        "# gather names of all images in the resized directory\n",
        "resized_images = os.listdir('resized_train_images/')\n",
        "\n",
        "images = []\n",
        "# find corresponding image from the resized directory to the selected sample found\n",
        "# in dictionary and add that to a list\n",
        "for image in df_dict.keys():\n",
        "  if image in resized_images:\n",
        "    img_resized = cv2.imread('resized_train_images/' + image) \n",
        "    images.append(img_resized)"
      ],
      "metadata": {
        "id": "ksV1F9z-I7xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert image list to array\n",
        "images = np.array(images)\n",
        "\n",
        "# convert dictionary values to array\n",
        "labels = np.array(list(df_dict.values()))\n",
        "\n",
        "# split data\n",
        "x_train, x_val, y_train, y_val = train_test_split(images, labels, test_size = 0.3, random_state=42)"
      ],
      "metadata": {
        "id": "o4A7xjIFAwyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building and training the network"
      ],
      "metadata": {
        "id": "2Zhu6lDvWjq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import layers, models, preprocessing\n",
        "\n",
        "def train_and_evaluate(model, train_x, train_y, val_x, val_y, preprocess={}, epochs=20, augment={}):\n",
        "\n",
        "    # optimizer = keras.optimizers.Adam(lr = 0.01)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
        "\n",
        "    train_gen = preprocessing.image.ImageDataGenerator(**preprocess, **augment)\n",
        "    train_gen.fit(train_x) \n",
        "\n",
        "    val_gen = preprocessing.image.ImageDataGenerator(**preprocess)\n",
        "    val_gen.fit(train_x)\n",
        "\n",
        "    history = model.fit(train_gen.flow(train_x, train_y), epochs=epochs, \n",
        "                        validation_data=val_gen.flow(val_x, val_y))\n",
        "\n",
        "    fig, axs = plt.subplots(1,2,figsize=(20,5)) \n",
        "\n",
        "    for i, metric in enumerate(['loss', 'accuracy']):\n",
        "        axs[i].plot(history.history[metric])\n",
        "        axs[i].plot(history.history['val_'+metric])\n",
        "        axs[i].legend(['training', 'validation'], loc='best')\n",
        "\n",
        "        axs[i].set_title('Model '+metric)\n",
        "        axs[i].set_ylabel(metric)\n",
        "        axs[i].set_xlabel('epoch')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Validation Accuracy: {model.evaluate(val_gen.flow(val_x, val_y))[1]}\")"
      ],
      "metadata": {
        "id": "2uvdJ8Yh3W7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML MODEL ARCHITECTURE\n",
        "# Define Sequential model\n",
        "model = models.Sequential()\n",
        "\n",
        "# create convolutional layer and max pooling layer\n",
        "model.add(layers.Conv2D(32, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.5), padding='same', input_shape=(96, 96, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# create convolutional layer (larger) and max pooling layer\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.5), padding='same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "# add Conv2D layer with 128 filters and max pooling layer\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.5), padding='same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "# add Conv2D layer with 256 filters and max pooling layer\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(layers.Conv2D(256, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.5), padding='same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# add Conv2D layer with 32 filters and max pooling layer\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.5), padding='same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# flatten layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(layers.Dense(256, activation=tf.keras.layers.LeakyReLU(alpha=0.5)))\n",
        "\n",
        "# apply softmax activation for final layer classification\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(layers.Dense(6, activation='softmax'))\n",
        "\n",
        "# normalize input data: set preprocesing dictionary\n",
        "preprocess = {'featurewise_center': True, 'featurewise_std_normalization' : True}\n",
        "\n",
        "# augment data: set augmentation dictionary\n",
        "augment = {'horizontal_flip': True, \n",
        "           'vertical_flip': True, \n",
        "           'rotation_range': 20, \n",
        "           'width_shift_range': 0.1, \n",
        "           'height_shift_range': 0.1, \n",
        "           'zoom_range': [0.3,1.0], \n",
        "           'brightness_range': [0.2,1.2],\n",
        "           'channel_shift_range' : 0.7}\n",
        "\n",
        "# run training and evaluation function\n",
        "train_and_evaluate(model, x_train, y_train, x_val, y_val, preprocess, epochs = 80, augment = augment)\n"
      ],
      "metadata": {
        "id": "ULlZim8l2zKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "aEjUnYrJ6u2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix"
      ],
      "metadata": {
        "id": "lvYNMVddWur6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select classes\n",
        "classes = df['adjusted labels'].unique()\n",
        "display(classes)\n",
        "\n",
        "# gather actual and predicted classes\n",
        "y_true = tf.argmax(y_val, axis=1)\n",
        "# y_pred = tf.keras.utils.to_categorical(y_pred, num_classes = 6)\n",
        "y_pred = tf.argmax(model.predict(x_val), axis = 1)\n",
        "\n",
        "# plot confusion matrix 1\n",
        "conf_matrix = tfmath.confusion_matrix(y_true, y_pred, num_classes = 6)\n",
        "\n",
        "ax = sns.heatmap(conf_matrix, xticklabels=classes, yticklabels=classes)\n",
        "ax.set(xlabel='Predicted Class', ylabel='Actual Class')\n",
        "plt.show()\n",
        "\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "jcwt60b_Jtc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot confusion matrix 2\n",
        "mtrx = confusion_matrix(y_true, y_pred)\n",
        "plot_confusion_matrix(conf_mat = mtrx, figsize=(8, 8), class_names=classes, colorbar=True, show_normed = True)"
      ],
      "metadata": {
        "id": "wRrn2C6vNU6q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}