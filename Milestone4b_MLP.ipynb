{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Milestone3_MLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeLaKo/apple-tree-disease/blob/main/Milestone4b_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the data and importing the necessary libraries"
      ],
      "metadata": {
        "id": "TqzOGBFWtYKn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NF6qQ5qUmQI_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec36220f-3e34-460f-9390-4bd77d70e4c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import math as tfmath\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "ibOekLk5tlko"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import mlxtend package for confusion matrix\n",
        "import mlxtend\n",
        "                                                          \n",
        "print(mlxtend.__version__) \n",
        "\n",
        "! pip install mlxtend --upgrade --no-deps\n",
        "\n",
        "print(mlxtend.__version__) \n",
        "\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dHkG28jM9N9",
        "outputId": "bc873996-1bb6-4eac-f2b1-a3d1e8dc184d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.19.0\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.7/dist-packages (0.19.0)\n",
            "0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Only run the following cells if data needs to be newly downloaded !\n",
        "'''\n",
        "# !pip install -U -q kaggle==1.5.8"
      ],
      "metadata": {
        "id": "ciuScTfXr0VL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3fda81dd-d22c-4338-dc43-45dfef7a44b7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nOnly run the following cells if data needs to be newly downloaded !\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/gdrive/MyDrive/MLP/"
      ],
      "metadata": {
        "id": "CFd-yrB9XWzT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/MLP/\"\n",
        "# ! kaggle competitions download -c plant-pathology-2021-fgvc8"
      ],
      "metadata": {
        "id": "XIrQYT90uv5m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! mkdir train"
      ],
      "metadata": {
        "id": "CnpAjve8XtVx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! unzip -q plant-pathology-2021-fgvc8.zip -d train"
      ],
      "metadata": {
        "id": "4w4s3qRQ3Z6X"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data processing\n"
      ],
      "metadata": {
        "id": "v192m162tVHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/MLP/train/"
      ],
      "metadata": {
        "id": "IesKjmS1cNkU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1152085e-4600-420f-8669-e908a46058e7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/MLP/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read training data\n",
        "df = pd.read_csv('train.csv')\n",
        "print(df.head(2))"
      ],
      "metadata": {
        "id": "4oGL6rPetoLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e885c781-cb3d-443f-8e9c-89c4138a1159"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  image                           labels\n",
            "0  800113bb65efe69e.jpg                          healthy\n",
            "1  8002cb321f8bfcdf.jpg  scab frog_eye_leaf_spot complex\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove all combined classes from dataset\n",
        "df = df.set_index('labels')\n",
        "df = df.drop('scab frog_eye_leaf_spot complex', axis = 0)\n",
        "df = df.drop('scab frog_eye_leaf_spot', axis = 0)\n",
        "df = df.drop('frog_eye_leaf_spot complex', axis = 0)\n",
        "df = df.drop('powdery_mildew complex', axis = 0)\n",
        "df = df.drop('rust complex', axis = 0)\n",
        "df = df.drop('rust frog_eye_leaf_spot', axis = 0)\n",
        "df = df.drop('complex', axis = 0)\n",
        "df = df.reset_index()\n",
        "print(df.head())\n",
        "\n",
        "new_labels = df['labels'].to_list()\n",
        "df['adjusted labels'] = np.array(new_labels)\n",
        "df = df.drop('labels', axis = 1)"
      ],
      "metadata": {
        "id": "wG1eHin2j2Nu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0ae9b0c-9d93-4e7d-c04f-4b786704072a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    labels                 image\n",
            "0  healthy  800113bb65efe69e.jpg\n",
            "1     scab  80070f7fb5e2ccaa.jpg\n",
            "2     scab  80077517781fb94f.jpg\n",
            "3  healthy  800edef467d27c15.jpg\n",
            "4     rust  800f85dc5f407aef.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(df):\n",
        "  \"\"\"\n",
        "  This function returns all classes and combination of classes found in the input-dataframe, and returns\n",
        "  the one-hot encoded version\n",
        "  \"\"\"\n",
        "  one_hot = pd.get_dummies(df['adjusted labels'])\n",
        "  df = df.drop('adjusted labels', axis = 1)\n",
        "  df = df.join(one_hot)\n",
        "  return df\n",
        "\n",
        "def sample(df, sample_size):\n",
        "  \"\"\"\n",
        "  This function gets an equal sample inclusive of all classes from the input dataframe\n",
        "  \"\"\"\n",
        "  df_sampled = []\n",
        "  classes = df['adjusted labels'].unique()\n",
        "\n",
        "  for i in classes:\n",
        "      g = df[df['adjusted labels'] == i].sample(sample_size)\n",
        "      df_sampled.append(g)\n",
        "\n",
        "  df_sampled = pd.concat(df_sampled)\n",
        "  return df_sampled"
      ],
      "metadata": {
        "id": "6Wh0KDFsuTER"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Run this cell if network is trained using all of the data !\n",
        "'''\n",
        "# df_onehot = one_hot(df)\n",
        "# print(df_onehot.head(2))"
      ],
      "metadata": {
        "id": "c2jRKcu2pXVU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d97660df-a111-4cb4-d874-b74bbf9195cc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nRun this cell if network is trained using all of the data !\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Run this cell if network is trained with only a sample !\n",
        "'''\n",
        "# select sample from dataframe\n",
        "df_sampled = sample(df, 1184)\n",
        "print(df_sampled.head(2))\n",
        "\n",
        "# convert labels within dataframe to one-hot encoded classes\n",
        "df_onehot = one_hot(df_sampled)\n",
        "print(df_onehot.head(2))"
      ],
      "metadata": {
        "id": "Zh7Rr6dNzlMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1f55096-d4eb-4788-a393-9ad40b268117"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      image adjusted labels\n",
            "12452  e871c3ced550aaaa.jpg         healthy\n",
            "3612   a1d196b94b8f6d60.jpg         healthy\n",
            "                      image  frog_eye_leaf_spot  ...  rust  scab\n",
            "12452  e871c3ced550aaaa.jpg                   0  ...     0     0\n",
            "3612   a1d196b94b8f6d60.jpg                   0  ...     0     0\n",
            "\n",
            "[2 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image-data processing"
      ],
      "metadata": {
        "id": "R3ibsTmfcdxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Only run this cell if images need resizing !\n",
        "'''\n",
        "# # create new directory\n",
        "# ! mkdir resized_train_images\n",
        "\n",
        "# # gather names of all images in the image-directory\n",
        "# train_images = os.listdir('train_images/')\n",
        "\n",
        "# # resize all images and save it to a new directory \n",
        "# for image in train_images:\n",
        "#   img = cv2.imread('train_images/' + image)\n",
        "#   resized_img = cv2.resize(img, (96, 96)) \n",
        "#   cv2.imwrite('resized_train_images/' + image, resized_img)\n"
      ],
      "metadata": {
        "id": "4jts4lwY0YS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert dataframe to a hashable list; dictionary\n",
        "df_dict = df_onehot.set_index('image').T.to_dict('list')\n",
        "\n",
        "# gather names of all images in the resized directory\n",
        "resized_images = os.listdir('resized_train_images/')\n",
        "\n",
        "images = []\n",
        "# find corresponding image from the resized directory to the selected sample found\n",
        "# in dictionary and add that to a list\n",
        "for image in df_dict.keys():\n",
        "  if image in resized_images:\n",
        "    img_resized = cv2.imread('resized_train_images/' + image) \n",
        "    images.append(img_resized)"
      ],
      "metadata": {
        "id": "ksV1F9z-I7xn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert image list to array\n",
        "images = np.array(images)\n",
        "\n",
        "# convert dictionary values to array\n",
        "labels = np.array(list(df_dict.values()))\n",
        "\n",
        "# split data\n",
        "x_train, x_val, y_train, y_val = train_test_split(images, labels, test_size = 0.3, random_state=42)"
      ],
      "metadata": {
        "id": "o4A7xjIFAwyb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building and training the network"
      ],
      "metadata": {
        "id": "2Zhu6lDvWjq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import layers, models, preprocessing\n",
        "\n",
        "def train_and_evaluate(model, train_x, train_y, val_x, val_y, preprocess={}, epochs=20, augment={}, class_weight = {}):\n",
        "\n",
        "    # optimizer = keras.optimizers.Adam(lr = 0.01)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
        "\n",
        "    train_gen = preprocessing.image.ImageDataGenerator(**preprocess, **augment)\n",
        "    train_gen.fit(train_x) \n",
        "\n",
        "    val_gen = preprocessing.image.ImageDataGenerator(**preprocess)\n",
        "    val_gen.fit(train_x)\n",
        "\n",
        "    history = model.fit(train_gen.flow(train_x, train_y), epochs=epochs, \n",
        "                        validation_data=val_gen.flow(val_x, val_y), class_weight = class_weight)\n",
        "\n",
        "    fig, axs = plt.subplots(1,2,figsize=(20,5)) \n",
        "\n",
        "    for i, metric in enumerate(['loss', 'accuracy']):\n",
        "        axs[i].plot(history.history[metric])\n",
        "        axs[i].plot(history.history['val_'+metric])\n",
        "        axs[i].legend(['training', 'validation'], loc='best')\n",
        "\n",
        "        axs[i].set_title('Model '+metric)\n",
        "        axs[i].set_ylabel(metric)\n",
        "        axs[i].set_xlabel('epoch')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Validation Accuracy: {model.evaluate(val_gen.flow(val_x, val_y))[1]}\")"
      ],
      "metadata": {
        "id": "2uvdJ8Yh3W7g"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute class-weights for inbalanced data\n",
        "y_integers = np.argmax(y_train, axis=1)\n",
        "class_weights = class_weight.compute_class_weight(class_weight = 'balanced', classes = np.unique(y_integers), y = y_integers)\n",
        "class_weight = dict(enumerate(class_weights))"
      ],
      "metadata": {
        "id": "7O9g12ud2Yjh"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML MODEL ARCHITECTURE\n",
        "# Define Sequential model\n",
        "model = models.Sequential()\n",
        "\n",
        "# create convolutional layer and max pooling layer\n",
        "model.add(layers.Conv2D(32, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.5), padding='same', input_shape=(96, 96, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# create convolutional layer (larger) and max pooling layer\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.5), padding='same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "# add Conv2D layer with 128 filters and max pooling layer\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.5), padding='same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "# add Conv2D layer with 256 filters and max pooling layer\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(layers.Conv2D(256, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.5), padding='same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# add Conv2D layer with 32 filters and max pooling layer\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.5), padding='same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# flatten layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(layers.Dense(256, activation=tf.keras.layers.LeakyReLU(alpha=0.5)))\n",
        "\n",
        "# apply softmax activation for final layer classification\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(layers.Dense(5, activation='softmax'))\n",
        "\n",
        "# normalize input data: set preprocesing dictionary\n",
        "preprocess = {'featurewise_center': True, 'featurewise_std_normalization' : True}\n",
        "\n",
        "# augment data: set augmentation dictionary\n",
        "augment = {'horizontal_flip': True, \n",
        "           'vertical_flip': True, \n",
        "           'rotation_range': 20, \n",
        "           'width_shift_range': 0.1, \n",
        "           'height_shift_range': 0.1, \n",
        "           'zoom_range': [0,1.5], \n",
        "           'brightness_range': [0,1.5],\n",
        "           'channel_shift_range' : 0.9,\n",
        "           'shear_range' : 0.9}\n",
        "\n",
        "# run training and evaluation function\n",
        "train_and_evaluate(model, x_train, y_train, x_val, y_val, preprocess, epochs = 80, augment = augment, class_weight = class_weight)\n"
      ],
      "metadata": {
        "id": "ULlZim8l2zKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab063d4-f4b0-4656-d609-9e9e677c6101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "130/130 [==============================] - 20s 134ms/step - loss: 2.2696 - accuracy: 0.2370 - val_loss: 1.4935 - val_accuracy: 0.3508\n",
            "Epoch 2/80\n",
            "130/130 [==============================] - 17s 133ms/step - loss: 1.8194 - accuracy: 0.2734 - val_loss: 1.4541 - val_accuracy: 0.3412\n",
            "Epoch 3/80\n",
            "130/130 [==============================] - 18s 135ms/step - loss: 1.7585 - accuracy: 0.2679 - val_loss: 1.4452 - val_accuracy: 0.3682\n",
            "Epoch 4/80\n",
            "130/130 [==============================] - 18s 140ms/step - loss: 1.6742 - accuracy: 0.2862 - val_loss: 1.4717 - val_accuracy: 0.2967\n",
            "Epoch 5/80\n",
            "130/130 [==============================] - 17s 132ms/step - loss: 1.6315 - accuracy: 0.2942 - val_loss: 1.5063 - val_accuracy: 0.3688\n",
            "Epoch 6/80\n",
            "130/130 [==============================] - 18s 135ms/step - loss: 1.5943 - accuracy: 0.3002 - val_loss: 1.4065 - val_accuracy: 0.4116\n",
            "Epoch 7/80\n",
            "130/130 [==============================] - 17s 133ms/step - loss: 1.5714 - accuracy: 0.3019 - val_loss: 1.3534 - val_accuracy: 0.4026\n",
            "Epoch 8/80\n",
            "130/130 [==============================] - 17s 134ms/step - loss: 1.5740 - accuracy: 0.3091 - val_loss: 1.3836 - val_accuracy: 0.3986\n",
            "Epoch 9/80\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 1.5304 - accuracy: 0.3152 - val_loss: 1.3545 - val_accuracy: 0.4105\n",
            "Epoch 10/80\n",
            "130/130 [==============================] - 25s 191ms/step - loss: 1.5164 - accuracy: 0.3214 - val_loss: 1.4057 - val_accuracy: 0.3694\n",
            "Epoch 11/80\n",
            "130/130 [==============================] - 18s 135ms/step - loss: 1.5110 - accuracy: 0.3308 - val_loss: 1.5619 - val_accuracy: 0.3423\n",
            "Epoch 12/80\n",
            "130/130 [==============================] - 18s 135ms/step - loss: 1.4764 - accuracy: 0.3533 - val_loss: 1.4764 - val_accuracy: 0.3818\n",
            "Epoch 13/80\n",
            "130/130 [==============================] - 17s 133ms/step - loss: 1.4681 - accuracy: 0.3557 - val_loss: 1.3597 - val_accuracy: 0.4116\n",
            "Epoch 14/80\n",
            "130/130 [==============================] - 17s 132ms/step - loss: 1.4739 - accuracy: 0.3489 - val_loss: 1.2832 - val_accuracy: 0.4279\n",
            "Epoch 15/80\n",
            "130/130 [==============================] - 17s 132ms/step - loss: 1.4522 - accuracy: 0.3518 - val_loss: 1.3790 - val_accuracy: 0.4003\n",
            "Epoch 16/80\n",
            "130/130 [==============================] - 18s 139ms/step - loss: 1.4429 - accuracy: 0.3612 - val_loss: 1.2547 - val_accuracy: 0.4628\n",
            "Epoch 17/80\n",
            "130/130 [==============================] - 17s 134ms/step - loss: 1.4335 - accuracy: 0.3779 - val_loss: 1.3014 - val_accuracy: 0.4336\n",
            "Epoch 18/80\n",
            "130/130 [==============================] - 17s 132ms/step - loss: 1.4111 - accuracy: 0.3875 - val_loss: 1.6413 - val_accuracy: 0.3834\n",
            "Epoch 19/80\n",
            "130/130 [==============================] - 18s 137ms/step - loss: 1.4258 - accuracy: 0.3709 - val_loss: 1.4208 - val_accuracy: 0.3885\n",
            "Epoch 20/80\n",
            "130/130 [==============================] - 18s 135ms/step - loss: 1.4211 - accuracy: 0.3909 - val_loss: 1.3052 - val_accuracy: 0.4358\n",
            "Epoch 21/80\n",
            "130/130 [==============================] - 18s 139ms/step - loss: 1.3895 - accuracy: 0.4059 - val_loss: 1.1878 - val_accuracy: 0.4870\n",
            "Epoch 22/80\n",
            "130/130 [==============================] - 17s 134ms/step - loss: 1.3825 - accuracy: 0.4122 - val_loss: 1.2269 - val_accuracy: 0.4887\n",
            "Epoch 23/80\n",
            "130/130 [==============================] - 17s 133ms/step - loss: 1.3810 - accuracy: 0.4112 - val_loss: 1.2522 - val_accuracy: 0.4690\n",
            "Epoch 24/80\n",
            "130/130 [==============================] - 17s 131ms/step - loss: 1.3693 - accuracy: 0.4100 - val_loss: 1.3560 - val_accuracy: 0.4414\n",
            "Epoch 25/80\n",
            "130/130 [==============================] - 18s 138ms/step - loss: 1.3675 - accuracy: 0.4100 - val_loss: 1.1063 - val_accuracy: 0.5045\n",
            "Epoch 26/80\n",
            "130/130 [==============================] - 17s 133ms/step - loss: 1.3784 - accuracy: 0.4047 - val_loss: 1.1857 - val_accuracy: 0.5045\n",
            "Epoch 27/80\n",
            "130/130 [==============================] - 18s 136ms/step - loss: 1.3673 - accuracy: 0.4117 - val_loss: 1.2780 - val_accuracy: 0.4465\n",
            "Epoch 28/80\n",
            "130/130 [==============================] - 17s 132ms/step - loss: 1.3601 - accuracy: 0.4218 - val_loss: 1.0860 - val_accuracy: 0.5360\n",
            "Epoch 29/80\n",
            "130/130 [==============================] - 17s 134ms/step - loss: 1.3529 - accuracy: 0.4218 - val_loss: 1.1608 - val_accuracy: 0.4916\n",
            "Epoch 30/80\n",
            "130/130 [==============================] - 17s 132ms/step - loss: 1.3534 - accuracy: 0.4192 - val_loss: 1.1552 - val_accuracy: 0.5265\n",
            "Epoch 31/80\n",
            "130/130 [==============================] - 17s 134ms/step - loss: 1.3155 - accuracy: 0.4633 - val_loss: 1.1554 - val_accuracy: 0.5591\n",
            "Epoch 32/80\n",
            "130/130 [==============================] - 19s 144ms/step - loss: 1.3339 - accuracy: 0.4455 - val_loss: 1.0802 - val_accuracy: 0.5484\n",
            "Epoch 33/80\n",
            " 18/130 [===>..........................] - ETA: 22s - loss: 1.3234 - accuracy: 0.4393"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "aEjUnYrJ6u2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix"
      ],
      "metadata": {
        "id": "lvYNMVddWur6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select classes\n",
        "classes = df['adjusted labels'].unique()\n",
        "display(classes)\n",
        "\n",
        "# gather actual and predicted classes\n",
        "y_true = tf.argmax(y_val, axis=1)\n",
        "y_pred = tf.argmax(model.predict(x_val), axis = 1)\n",
        "\n",
        "# plot confusion matrix 1\n",
        "conf_matrix = tfmath.confusion_matrix(y_true, y_pred, num_classes = 5)\n",
        "\n",
        "ax = sns.heatmap(conf_matrix, xticklabels=classes, yticklabels=classes)\n",
        "ax.set(xlabel='Predicted Class', ylabel='Actual Class')\n",
        "plt.show()\n",
        "\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "jcwt60b_Jtc_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "outputId": "e1144784-ee61-4c8b-b945-9365147da507"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array(['healthy', 'scab', 'rust', 'frog_eye_leaf_spot', 'powdery_mildew'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[  2   0   0   1 353]\n",
            " [  1   0   0   0 382]\n",
            " [  0   0   0   0 340]\n",
            " [  3   0   0   0 344]\n",
            " [  3   0   0   0 347]], shape=(5, 5), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot confusion matrix 2\n",
        "mtrx = confusion_matrix(y_true, y_pred)\n",
        "plot_confusion_matrix(conf_mat = mtrx, figsize=(8, 8), class_names=classes, colorbar=True, show_normed = True)"
      ],
      "metadata": {
        "id": "wRrn2C6vNU6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # VGG-16 model\n",
        "\n",
        "# # Define Sequential model\n",
        "# modelVGG_16 = models.Sequential()\n",
        "\n",
        "# # create convolutional layers and max pooling layer\n",
        "# modelVGG_16.add(layers.Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\",input_shape=(96, 96, 3)))\n",
        "# modelVGG_16.add(layers.Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "# modelVGG_16.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# # create convolutional layers and max pooling layer\n",
        "# modelVGG_16.add(layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "# modelVGG_16.add(layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "# modelVGG_16.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# # create convolutional layers and max pooling layer\n",
        "# modelVGG_16.add(layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "# modelVGG_16.add(layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "# modelVGG_16.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# # create convolutional layers and max pooling layer\n",
        "# modelVGG_16.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "# modelVGG_16.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "# modelVGG_16.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "# modelVGG_16.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# # # create convolutional layers and max pooling layer\n",
        "# # modelVGG_16.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "# # modelVGG_16.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "# # modelVGG_16.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "# # modelVGG_16.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# # flatten layer\n",
        "# modelVGG_16.add(layers.Flatten())\n",
        "# modelVGG_16.add(layers.Dense(4096, activation=\"relu\"))\n",
        "# modelVGG_16.add(tf.keras.layers.Dropout(0.5))\n",
        "# modelVGG_16.add(layers.Dense(4096, activation=\"relu\"))\n",
        "# modelVGG_16.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "# # predict\n",
        "# modelVGG_16.add(layers.Dense(6, activation='softmax'))\n",
        "\n",
        "# # normalize input data: set preprocesing dictionary\n",
        "# preprocess = {'featurewise_center': True, 'featurewise_std_normalization' : True}\n",
        "\n",
        "# # augment data: set augmentation dictionary\n",
        "# augment = {'horizontal_flip': True, \n",
        "#            'vertical_flip': True, \n",
        "#            'rotation_range': 20, \n",
        "#            'width_shift_range': 0.1, \n",
        "#            'height_shift_range': 0.1, \n",
        "#            'zoom_range': [0,1.5], \n",
        "#            'brightness_range': [0,1.5],\n",
        "#            'channel_shift_range' : 0.9,\n",
        "#            'shear_range' : 0.9}\n",
        "\n",
        "# # run training and evaluation function\n",
        "# train_and_evaluate(modelVGG_16, x_train, y_train, x_val, y_val, preprocess, epochs = 80, augment = augment)"
      ],
      "metadata": {
        "id": "Bdg6-4hpLkIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modelVGG_16.summary()"
      ],
      "metadata": {
        "id": "guoUhcmEVwvp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}